# Sign Language Recognition

This project is an **American Sign Language (ASL) recognition system** that uses **Convolutional Neural Networks (CNNs)** to translate ASL signs into text in real time.

## ðŸ“Œ Features
- Real-time ASL recognition from video input.
- Uses a **self-made dataset** for training.
- Implements **CNN-based deep learning model**.
- Provides accurate text translation for ASL gestures.

## ðŸ“‚ Project Structure
```
proj/
â”œâ”€â”€ .ipynb_checkpoints/   # Jupyter notebook checkpoints
â”œâ”€â”€ data/                 # Self-made ASL dataset
â”œâ”€â”€ CNN_Model.ipynb       # CNN model implementation
â”œâ”€â”€ app.ipynb             # ASL recognition application
â”œâ”€â”€ collect_data.ipynb    # Script to collect ASL data
â”œâ”€â”€ model.h5              # Trained model weights
â”œâ”€â”€ model.json            # Model architecture
â”œâ”€â”€ preprocessing of data.ipynb  # Data preprocessing notebook
â”œâ”€â”€ testing.ipynb         # Model testing notebookpt
â””â”€â”€ README.md             # Project documentation
```

## ðŸ“Š Dataset
The dataset consists of **self-collected ASL gesture images**. You can expand it by adding more labeled data.

## ðŸ§  Model
The system employs a **CNN-based architecture** for ASL classification. The model is trained using **TensorFlow/Keras or PyTorch** (whichever you are using).

## ðŸ”¥ Future Improvements
- Improve model accuracy with a larger dataset.
- Support for dynamic ASL gestures.

### ðŸ”— Project Link
[GitHub Repository](https://github.com/viniskykumar/Sign_Language_Recognition)
